{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "TODO: Refer to dissertation chapter on why existing datasets are insufficient\n",
    "\n",
    "This notebooks trains a convolutional neural network (CNN) to segment UAV\n",
    "imagery into `ground`, `water`, and `building` classes. This process is called\n",
    "*semantic segmentation*.\n",
    "\n",
    "The purpose of this functionality for the Lakehopper system is twofold:\n",
    "- Provide awareness of the situation under the drone at the moment of flight\n",
    "  ('in-situ'). This is as opposed to beforehand (when a certain dataset was\n",
    "  captured or when the drone previously collected data). A lake might have dried\n",
    "  up or a boat might be present where it wasn't previously.\n",
    "- Provide a wide-area map based on orthographic imagery with features that are\n",
    "  relevant to lakehopper. Other maps do not necessarily have the same feature\n",
    "  definitions as lakehopper. The `water` class for Lakehopper is defined as 'a\n",
    "  body of surface water where autonomous landing is possible'. Swamps or water\n",
    "  obscured by bridges or vegetation do not fit this definition. They are however\n",
    "  still classified as 'water' on most topographic maps.\n",
    "  \n",
    "The dataset is too large to upload via cloud storage (58.6 GiB). Please contact pieter@pfiers.net for a copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "import sys, os, csv\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.distribute.tpu_strategy import TPUStrategy\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience methods\n",
    "from visualise import show\n",
    "from tpu import resolve_tpu_strategy, get_tpu_devices\n",
    "from dataset import load_dataset, split_dataset_paths, filter_out_only_ground_paths\n",
    "from masks import colorize_byte_mask\n",
    "from models.unet import create_unet\n",
    "from models.fcn import create_fcn8\n",
    "from models.fpn import create_fpn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up TPU\n",
    "\n",
    "To speed up model training, we use [Google Cloud Platform](https://cloud.google.com/)'s \"[TPU's](https://cloud.google.com/tpu)\". TPU's use a machine learning ASIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu_strategy = resolve_tpu_strategy('semseg-node-us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbro_tpu_devices = len(get_tpu_devices())\n",
    "print(f\"{nbro_tpu_devices} TPU devices connected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARALLEL_OPS = tf.data.AUTOTUNE\n",
    "# PARALLEL_OPS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (128, 128)\n",
    "GCS_PATTERN = 'gs://lakehopper-semseg-data/aggregated-28-7-tfr/*.tfr'\n",
    "paths = filter_out_only_ground_paths(tf.io.gfile.glob(GCS_PATTERN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.7\n",
    "VALIDATION_RATIO = 0.15\n",
    "train_paths, validate_paths, test_paths = split_dataset_paths(paths, TRAIN_RATIO, VALIDATION_RATIO)\n",
    "print(f\"Found {len(paths)} tfrs. Splitting into {len(train_paths)} training, {len(validate_paths)} validation, and {len(test_paths)} test tfrs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualisation_dataset = load_dataset(train_paths, IMAGE_SIZE, PARALLEL_OPS)\n",
    "sample_image, sample_label = next(islice(iter(visualisation_dataset), 0, None))\n",
    "show((sample_image, colorize_byte_mask(sample_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sample_weights(image, label):\n",
    "    # The weights for each class, with the constraint that:\n",
    "    #     sum(class_weights) == 1.0\n",
    "    class_weights = tf.constant([0.1, 0.01, 0.2])\n",
    "    class_weights = class_weights/tf.reduce_sum(class_weights)\n",
    "\n",
    "    # Create an image of `sample_weights` by using the label at each pixel as an \n",
    "    # index into the `class weights` .\n",
    "    sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
    "\n",
    "    return image, label, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augment(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rng = tf.random.Generator.from_seed(42, alg='philox')\n",
    "        \n",
    "    def call(self, image, labels, weights):\n",
    "        seeds = tf.random.experimental.stateless_split(self.rng.make_seeds(2)[0], num=3)\n",
    "        image = tf.image.stateless_random_flip_left_right(image, seeds[0])\n",
    "        image = tf.image.stateless_random_brightness(image, max_delta=0.5, seed=seeds[1])\n",
    "        image = tf.image.stateless_random_hue(image, max_delta=0.1, seed=seeds[2])\n",
    "        labels = tf.image.stateless_random_flip_left_right(labels, seeds[0])\n",
    "        weights = tf.image.stateless_random_flip_left_right(weights, seeds[0])\n",
    "        return image, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # Using TPU v3-8 device => must be divisible by 8 for sharding\n",
    "BUFFER_SIZE = 1000\n",
    "# BACKBONE = 'efficientnetb3'\n",
    "# preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "# Dataset generation *must* come after tpu resolution\n",
    "training_dataset = (\n",
    "    load_dataset(train_paths, IMAGE_SIZE, PARALLEL_OPS)\n",
    "    .map(add_sample_weights)\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .map(Augment())\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "validation_dataset = (\n",
    "    load_dataset(validate_paths, IMAGE_SIZE, PARALLEL_OPS)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nbro_classes: int, tpu_strategy: TPUStrategy = None) -> tf.keras.Model:\n",
    "    LR = 0.0001\n",
    "    ENCODER = 'InceptionResNetV2' # MobileNetV2  EfficientNetB3  ResNet50\n",
    "    \n",
    "    def scoped_create_model():\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        optimizer = tf.keras.optimizers.Adam(LR)\n",
    "        metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "        input_shape = IMAGE_SIZE + (3,)\n",
    "\n",
    "        model = create_unet(ENCODER, nbro_classes, input_shape)\n",
    "#         model = create_fpn(ENCODER, nbro_classes, input_shape)\n",
    "#         model = create_fcn8(ENCODER, nbro_classes, input_shape)\n",
    "        model.compile(ENCODER, optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "        return model\n",
    "\n",
    "    if tpu_strategy is not None:\n",
    "        with tpu_strategy.scope():\n",
    "            return scoped_create_model()\n",
    "    else:\n",
    "        return scoped_create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(3, tpu_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataset=[(sample_image, sample_label)]):\n",
    "    rows = [\n",
    "        (image, label, create_mask(model.predict(image[tf.newaxis, ...], verbose=0)))\n",
    "        for image, label in dataset\n",
    "    ]\n",
    "    show(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "VAL_SUBSPLITS = 2\n",
    "steps_per_epoch = len(train_paths) // BATCH_SIZE\n",
    "validation_steps = len(validate_paths) // BATCH_SIZE // VAL_SUBSPLITS\n",
    "\n",
    "print(f\"With a batch size of {BATCH_SIZE}, there will be {steps_per_epoch} batches per training epoch and {validation_steps} batches per validation run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_log_dir = \"gs://lakehopper-semseg-data/model/\" + datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=True, save_best_only=True, mode='min'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=tensorboard_log_dir, histogram_freq=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3),\n",
    "    DisplayCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    training_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "model.save('finished_model.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to csv\n",
    "datetimestr = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "with open(f'metrics-{datetimestr}', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['loss', 'val_loss'])\n",
    "    for train, val in zip(history.history['loss'], history.history['val_loss']):\n",
    "        writer.writerow([train, val])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "324c7903a9d09267d9b017a1da9a773868afe0a2e280c028e3fb1071dc1b94bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
